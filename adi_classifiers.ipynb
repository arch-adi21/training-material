{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Classification"
      ],
      "metadata": {
        "id": "NHQ7hRdmofDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Decision Trees :\n",
        "\n",
        "#### Decision Tree Construction:\n",
        "- **Objective:** Decision trees recursively split data based on features to make decisions.\n",
        "- **Splitting Criteria:** Decision trees use criteria like Gini impurity, entropy, or mean squared error to determine the best feature for splitting.\n",
        "- **Information Gain:** The decision tree algorithm seeks to maximize information gain at each split.\n",
        "\n",
        "\n",
        "\n",
        "Decision Trees are a popular machine learning algorithm used for both classification and regression tasks. They work by recursively splitting the dataset based on the most significant features, leading to a tree-like structure of decisions. Here are some relevant terminologies and explanations, along with coded examples in Python using scikit-learn.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "psbb7N7YokHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Relevant Terminologies:\n",
        "\n",
        "#### 1. Node:\n",
        "A node in a decision tree represents a decision or a test on a particular feature.\n",
        "\n",
        "#### 2. Root Node:\n",
        "The topmost node in the tree, representing the initial decision or test.\n",
        "\n",
        "#### 3. Leaf Node:\n",
        "A terminal node that contains the final decision or the predicted outcome.\n",
        "\n",
        "#### 4. Splitting:\n",
        "The process of dividing a node into two or more child nodes based on a certain criterion.\n",
        "\n",
        "#### 5. Feature:\n",
        "The input variable used for making decisions.\n",
        "\n",
        "#### 6. Criterion:\n",
        "The measure used to determine the quality of a split. Common criteria include Gini impurity, entropy, and mean squared error.\n",
        "\n",
        "#### 7. Decision Rule:\n",
        "The condition applied to a feature to make a decision. For example, \"If feature X is greater than Y, go left; otherwise, go right.\"\n",
        "\n",
        "#### 8. Pruning:\n",
        "The process of removing branches or nodes from a tree to prevent overfitting."
      ],
      "metadata": {
        "id": "1qKTV3NPoovO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Gini Impurity:\n",
        "\n",
        "One common criterion used in decision trees for classification tasks is the Gini impurity. For a given node $t $ with $ N_t $ samples, and $ k $ classes, the Gini impurity $ G(t) $ is calculated as:\n",
        "\n",
        "$ G(t) = 1 - \\sum_{i=1}^{k} p(i|t)^2 $\n",
        "\n",
        "Here:\n",
        "- $ p(i|t) $ is the proportion of samples of class $ i $ at node $ t $.\n",
        "\n",
        "The Gini impurity is a measure of how often a randomly chosen element would be incorrectly classified. It ranges from 0 (pure node) to 0.5 (maximum impurity).\n",
        "\n",
        "### 2. Information Gain:\n",
        "\n",
        "The decision tree algorithm aims to find splits that maximize information gain. Information gain is a measure of the effectiveness of a split in reducing uncertainty about the class labels. It is calculated as the difference in impurity before and after the split.\n",
        "\n",
        "$ \\text{Information Gain} = G(\\text{parent}) - \\sum_{i=1}^{m} \\frac{N_i}{N} G(\\text{child}_i) $\n",
        "\n",
        "Here:\n",
        "- $ G(\\text{parent}) $ is the Gini impurity of the parent node.\n",
        "- $ m $ is the number of child nodes after the split.\n",
        "- $ N_i $ is the number of samples in the $i$-th child node.\n",
        "- $ N $ is the total number of samples in the parent node.\n",
        "\n",
        "### 3. Decision Rule:\n",
        "\n",
        "The decision tree recursively selects the feature and split point that maximize information gain. The decision rule for a node is based on a feature $ X_j $ and a threshold value $ t $ :\n",
        "\n",
        "$ \\text{if } X_j \\leq t \\text{, go left; else, go right} $\n",
        "\n",
        "### 4. Entropy (Alternative Impurity Measure):\n",
        "\n",
        "An alternative impurity measure is entropy. The entropy $ H(t) $ of a node $ t $ is defined as:\n",
        "\n",
        "$ H(t) = -\\sum_{i=1}^{k} p(i|t) \\log_2 p(i|t) $\n",
        "\n",
        "Entropy measures the average information content needed to classify a sample. Decision trees can be built using either Gini impurity or entropy as the splitting criterion.\n",
        "\n"
      ],
      "metadata": {
        "id": "VFLUPb7bqMMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Practical Example (using Python and scikit-learn):\n",
        "\n",
        "This example shows how to train decision tree models with both Gini impurity and entropy as the criterion using scikit-learn.\n",
        "\n",
        "Understanding the underlying mathematics helps in appreciating how decision trees make decisions and the trade-offs involved in choosing different impurity measures.\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Create and train a Decision Tree model with Gini impurity\n",
        "tree_model_gini = DecisionTreeClassifier(criterion='gini')\n",
        "tree_model_gini.fit(X, y)\n",
        "\n",
        "# Create and train a Decision Tree model with entropy\n",
        "tree_model_entropy = DecisionTreeClassifier(criterion='entropy')\n",
        "tree_model_entropy.fit(X, y)\n",
        "\n",
        "# Visualize the decision tree rules for both models\n",
        "tree_rules_gini = export_text(tree_model_gini, feature_names=iris.feature_names)\n",
        "tree_rules_entropy = export_text(tree_model_entropy, feature_names=iris.feature_names)\n",
        "\n",
        "print(\"Decision Tree Rules (Gini impurity):\\n\", tree_rules_gini)\n",
        "print(\"\\nDecision Tree Rules (Entropy):\\n\", tree_rules_entropy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZsP5hdiqvPd",
        "outputId": "3362db41-013a-4a49-c60f-32dcd9ee8b57"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Rules (Gini impurity):\n",
            " |--- petal length (cm) <= 2.45\n",
            "|   |--- class: 0\n",
            "|--- petal length (cm) >  2.45\n",
            "|   |--- petal width (cm) <= 1.75\n",
            "|   |   |--- petal length (cm) <= 4.95\n",
            "|   |   |   |--- petal width (cm) <= 1.65\n",
            "|   |   |   |   |--- class: 1\n",
            "|   |   |   |--- petal width (cm) >  1.65\n",
            "|   |   |   |   |--- class: 2\n",
            "|   |   |--- petal length (cm) >  4.95\n",
            "|   |   |   |--- petal width (cm) <= 1.55\n",
            "|   |   |   |   |--- class: 2\n",
            "|   |   |   |--- petal width (cm) >  1.55\n",
            "|   |   |   |   |--- sepal length (cm) <= 6.95\n",
            "|   |   |   |   |   |--- class: 1\n",
            "|   |   |   |   |--- sepal length (cm) >  6.95\n",
            "|   |   |   |   |   |--- class: 2\n",
            "|   |--- petal width (cm) >  1.75\n",
            "|   |   |--- petal length (cm) <= 4.85\n",
            "|   |   |   |--- sepal length (cm) <= 5.95\n",
            "|   |   |   |   |--- class: 1\n",
            "|   |   |   |--- sepal length (cm) >  5.95\n",
            "|   |   |   |   |--- class: 2\n",
            "|   |   |--- petal length (cm) >  4.85\n",
            "|   |   |   |--- class: 2\n",
            "\n",
            "\n",
            "Decision Tree Rules (Entropy):\n",
            " |--- petal width (cm) <= 0.80\n",
            "|   |--- class: 0\n",
            "|--- petal width (cm) >  0.80\n",
            "|   |--- petal width (cm) <= 1.75\n",
            "|   |   |--- petal length (cm) <= 4.95\n",
            "|   |   |   |--- petal width (cm) <= 1.65\n",
            "|   |   |   |   |--- class: 1\n",
            "|   |   |   |--- petal width (cm) >  1.65\n",
            "|   |   |   |   |--- class: 2\n",
            "|   |   |--- petal length (cm) >  4.95\n",
            "|   |   |   |--- petal width (cm) <= 1.55\n",
            "|   |   |   |   |--- class: 2\n",
            "|   |   |   |--- petal width (cm) >  1.55\n",
            "|   |   |   |   |--- petal length (cm) <= 5.45\n",
            "|   |   |   |   |   |--- class: 1\n",
            "|   |   |   |   |--- petal length (cm) >  5.45\n",
            "|   |   |   |   |   |--- class: 2\n",
            "|   |--- petal width (cm) >  1.75\n",
            "|   |   |--- petal length (cm) <= 4.85\n",
            "|   |   |   |--- sepal length (cm) <= 5.95\n",
            "|   |   |   |   |--- class: 1\n",
            "|   |   |   |--- sepal length (cm) >  5.95\n",
            "|   |   |   |   |--- class: 2\n",
            "|   |   |--- petal length (cm) >  4.85\n",
            "|   |   |   |--- class: 2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding Decision Tree Rules:\n",
        "\n",
        "The output will look like a set of rules that represent the decision-making process of the tree. For example:\n",
        "\n",
        "```\n",
        "|--- petal width (cm) <= 0.80\n",
        "|   |--- class: 0\n",
        "|--- petal width (cm) >  0.80\n",
        "|   |--- petal width (cm) <= 1.75\n",
        "|   |   |--- petal length (cm) <= 4.95\n",
        "|   |   |   |--- class: 1\n",
        "|   |   |--- petal length (cm) >  4.95\n",
        "|   |   |   |--- class: 2\n",
        "|   |--- petal width (cm) >  1.75\n",
        "|   |   |--- petal length (cm) <= 4.85\n",
        "|   |   |   |--- class: 2\n",
        "|   |   |--- petal length (cm) >  4.85\n",
        "|   |   |   |--- class: 2\n",
        "```\n",
        "\n",
        "Each rule represents a decision based on a feature and its value, leading to a specific class prediction at the leaf nodes.\n",
        "\n",
        "In this example:\n",
        "- If petal width is less than or equal to 0.8, predict class 0.\n",
        "- If petal width is greater than 0.8 and petal width is less than or equal to 1.75:\n",
        "  - If petal length is less than or equal to 4.95, predict class 1.\n",
        "  - If petal length is greater than 4.95, predict class 2.\n",
        "- If petal width is greater than 1.75:\n",
        "  - If petal length is less than or equal to 4.85, predict class 2.\n",
        "  - If petal length is greater than 4.85, predict class 2.\n",
        "\n",
        "This is a simplified decision tree that captures the patterns in the data for the Iris classification task. In practice, decision trees can become more complex based on the complexity of the dataset.\n",
        "\n",
        "Remember, while decision trees are interpretable, they are also prone to overfitting. Techniques like pruning and limiting the maximum depth of the tree are often used to mitigate this."
      ],
      "metadata": {
        "id": "pjJY2DSwrNGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XxPp35pZpZqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree Example:\n",
        "\n",
        "# Let's create a simple decision tree using scikit-learn for a classification task:\n",
        "\n",
        "'''\n",
        "In this example, we use the Iris dataset, and the decision tree is trained to classify iris flowers into three classes\n",
        "(setosa, versicolor, and virginica). The `export_text` function is used to print the rules of the decision tree.\n",
        "'''\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree classifier\n",
        "classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = classifier.predict(X_test)\n",
        "\n",
        "# Display the predictions\n",
        "print(\"Predictions:\", predictions)\n",
        "\n",
        "# Display the actual labels\n",
        "print(\"Actual labels:\", y_test)\n",
        "\n",
        "# Calculate and display accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"\\nAccuracy:\", accuracy)\n",
        "\n",
        "# Display the decision tree rules\n",
        "tree_rules = export_text(classifier, feature_names=iris.feature_names)\n",
        "print(\"\\nDecision Tree Rules:\\n\", tree_rules)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXnkbYCHrUBP",
        "outputId": "a36b5857-e7ed-4820-ca2c-05cb742e9a5e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "Actual labels: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "\n",
            "Accuracy: 1.0\n",
            "\n",
            "Decision Tree Rules:\n",
            " |--- petal length (cm) <= 2.45\n",
            "|   |--- class: 0\n",
            "|--- petal length (cm) >  2.45\n",
            "|   |--- petal length (cm) <= 4.75\n",
            "|   |   |--- petal width (cm) <= 1.65\n",
            "|   |   |   |--- class: 1\n",
            "|   |   |--- petal width (cm) >  1.65\n",
            "|   |   |   |--- class: 2\n",
            "|   |--- petal length (cm) >  4.75\n",
            "|   |   |--- petal width (cm) <= 1.75\n",
            "|   |   |   |--- petal length (cm) <= 4.95\n",
            "|   |   |   |   |--- class: 1\n",
            "|   |   |   |--- petal length (cm) >  4.95\n",
            "|   |   |   |   |--- petal width (cm) <= 1.55\n",
            "|   |   |   |   |   |--- class: 2\n",
            "|   |   |   |   |--- petal width (cm) >  1.55\n",
            "|   |   |   |   |   |--- petal length (cm) <= 5.45\n",
            "|   |   |   |   |   |   |--- class: 1\n",
            "|   |   |   |   |   |--- petal length (cm) >  5.45\n",
            "|   |   |   |   |   |   |--- class: 2\n",
            "|   |   |--- petal width (cm) >  1.75\n",
            "|   |   |   |--- petal length (cm) <= 4.85\n",
            "|   |   |   |   |--- sepal width (cm) <= 3.10\n",
            "|   |   |   |   |   |--- class: 2\n",
            "|   |   |   |   |--- sepal width (cm) >  3.10\n",
            "|   |   |   |   |   |--- class: 1\n",
            "|   |   |   |--- petal length (cm) >  4.85\n",
            "|   |   |   |   |--- class: 2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble learning is a machine learning technique that involves combining multiple models to create a stronger and more robust predictive model. The fundamental idea behind ensemble learning is that by combining the predictions of multiple weak learners (individual models), the overall model's performance can be improved, often outperforming the individual models. Ensemble methods are widely used for both classification and regression tasks. There are various ensemble techniques, with the most common ones being Bagging, Boosting, and Stacking.\n",
        "\n",
        "### Key Concepts of Ensemble Learning:\n",
        "\n",
        "#### 1. **Weak Learners:**\n",
        "   - **Definition:** Weak learners are models that perform slightly better than random chance.\n",
        "   - **Example:** Decision trees with limited depth, linear models, or models with low complexity.\n",
        "\n",
        "#### 2. **Diversity:**\n",
        "   - **Importance:** Ensemble methods benefit from diverse weak learners, i.e., models that make errors on different subsets of the data.\n",
        "   - **Example:** Using different algorithms or tweaking hyperparameters to introduce diversity.\n",
        "\n",
        "#### 3. **Aggregation:**\n",
        "   - **Combining Predictions:** Ensemble methods aggregate the predictions of weak learners to make a final prediction.\n",
        "   - **Methods:** Common aggregation techniques include averaging (for regression) and voting (for classification).\n",
        "\n",
        "### Types of Ensemble Learning:\n",
        "\n",
        "#### 1. **Bagging (Bootstrap Aggregating):**\n",
        "   - **Idea:** Train multiple instances of the same model on different bootstrap samples (random subsets with replacement) of the training data.\n",
        "   - **Example:** Random Forest is a popular bagging ensemble method that uses decision trees.\n",
        "\n",
        "#### 2. **Boosting:**\n",
        "   - **Idea:** Train a sequence of weak learners, where each model corrects the errors of the previous one.\n",
        "   - **Example:** AdaBoost (Adaptive Boosting), Gradient Boosting Machines (GBM), XGBoost, and LightGBM are boosting algorithms.\n",
        "\n",
        "#### 3. **Stacking:**\n",
        "   - **Idea:** Train multiple diverse models and use another model (meta-learner) to combine their predictions.\n",
        "   - **Example:** Train various models like decision trees, support vector machines, and neural networks, and then use a meta-learner (e.g., linear regression) to combine their outputs.\n",
        "\n",
        "#### 4. **Random Forest:**\n",
        "   - **Composition:** A Random Forest is an ensemble of decision trees, where each tree is trained on a random subset of features and a random subset of the training data.\n",
        "   - **Benefits:** Reduces overfitting and improves generalization.\n",
        "\n",
        "### Benefits of Ensemble Learning:\n",
        "\n",
        "#### 1. **Improved Accuracy:**\n",
        "   - Ensemble methods often achieve higher accuracy compared to individual models, especially when dealing with complex datasets.\n",
        "\n",
        "#### 2. **Robustness:**\n",
        "   - Ensemble models are more robust to outliers and noisy data, as the errors of individual models can be balanced out.\n",
        "\n",
        "#### 3. **Generalization:**\n",
        "   - Ensemble methods tend to generalize well to new, unseen data, reducing the risk of overfitting.\n",
        "\n",
        "#### 4. **Versatility:**\n",
        "   - Ensemble learning can be applied to a wide range of machine learning algorithms, making it a versatile approach.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This example demonstrates the use of a Random Forest, a popular ensemble method, on the Iris dataset using scikit-learn. The Random Forest combines multiple decision trees to improve predictive performance."
      ],
      "metadata": {
        "id": "Jkl9Lbn-thd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Random Forest:**\n",
        "\n",
        "Random Forest is an ensemble learning algorithm that can be used for both classification and regression tasks. It was introduced by Leo Breiman and Adele Cutler. The key idea behind the Random Forest algorithm is to build a multitude of decision trees during training and output the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
        "\n",
        "**Key Components of Random Forest:**\n",
        "\n",
        "1. **Decision Trees:**\n",
        "   Random Forest is essentially a collection of decision trees. Decision trees are simple models that make decisions based on input features. Each tree in the Random Forest is trained on a subset of the data and a random subset of features.\n",
        "\n",
        "2. **Bootstrapping:**\n",
        "   During the training process, Random Forest uses a technique called bootstrapping. This involves randomly sampling the dataset with replacement to create multiple subsets, each of which is used to train a different decision tree.\n",
        "\n",
        "3. **Random Feature Selection:**\n",
        "   When training each tree, only a random subset of features is considered at each split point. This helps to decorrelate the trees, making the ensemble more robust and less prone to overfitting.\n",
        "\n",
        "4. **Voting or Averaging:**\n",
        "   For classification tasks, the final prediction is often determined by a majority vote among the individual trees. For regression tasks, the final prediction is the average prediction of all the trees.\n",
        "\n",
        "**Advantages of Random Forest:**\n",
        "\n",
        "1. **High Accuracy:**\n",
        "   Random Forest often provides high accuracy compared to individual decision trees, especially when the dataset is complex.\n",
        "\n",
        "2. **Robust to Overfitting:**\n",
        "   The random feature selection and bootstrapping help in creating diverse trees, reducing the risk of overfitting.\n",
        "\n",
        "3. **Handle Missing Values:**\n",
        "   Random Forest can handle missing values in the dataset and maintain good predictive performance.\n",
        "\n",
        "4. **Feature Importance:**\n",
        "   Random Forest provides a feature importance score, indicating which features are more influential in making predictions.\n",
        "\n",
        "5. **Parallelization:**\n",
        "   Training and prediction with Random Forest can be easily parallelized, making it computationally efficient.\n",
        "\n",
        "**Disadvantages of Random Forest:**\n",
        "\n",
        "1. **Less Interpretable:**\n",
        "   The ensemble nature of Random Forest makes it less interpretable compared to individual decision trees.\n",
        "\n",
        "2. **Computational Complexity:**\n",
        "   Training multiple decision trees can be computationally expensive, especially for large datasets.\n",
        "\n",
        "Overall, Random Forest is a powerful and versatile algorithm that is widely used in various machine learning applications due to its robustness and high performance."
      ],
      "metadata": {
        "id": "G7H1gOtBu0Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a Random Forest classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l2nvohNv4wU",
        "outputId": "317dfe5b-19e4-43ec-8d87-947405734498"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Support Vector Machines\n",
        "\n",
        "**Support Vector Machines (SVM):**\n",
        "\n",
        "- **Introduction:**\n",
        "  - A powerful supervised learning algorithm used for both classification and regression tasks.\n",
        "  - Particularly effective in high-dimensional spaces.\n",
        "\n",
        "- **Objective:**\n",
        "  - To find the hyperplane that best separates data into different classes.\n",
        "  - Maximizes the margin between classes, i.e., the distance between the hyperplane and the nearest data points (support vectors).\n",
        "\n",
        "- **Linear SVMs:**\n",
        "  - **Aim:** To find a linear hyperplane that maximally separates classes.\n",
        "  - **Approach:**\n",
        "    - Utilize a linear kernel function to map data into a higher-dimensional space.\n",
        "    - Seek the optimal hyperplane in this transformed space.\n",
        "  - **Advantages:**\n",
        "    - Effective for linearly separable datasets.\n",
        "    - Computational efficiency in high-dimensional spaces.\n",
        "\n",
        "- **Nonlinear SVMs:**\n",
        "  - **Purpose:** Handle datasets that are not linearly separable.\n",
        "  - **Approach:**\n",
        "    - Employ the kernel trick to implicitly map data into a higher-dimensional space.\n",
        "    - Allows finding a hyperplane in this transformed space, even when the original data is not linearly separable.\n",
        "  - **Kernel Trick:**\n",
        "    - A mathematical method that avoids explicitly computing the coordinates of data points in the higher-dimensional space.\n",
        "    - Popular kernel functions include polynomial, radial basis function (RBF), and sigmoid kernels.\n",
        "  - **Advantages:**\n",
        "    - Flexibility to capture complex relationships between features and labels.\n",
        "    - Can handle both linear and nonlinear relationships effectively.\n",
        "\n",
        "- **Applications:**\n",
        "  - Widely used in various fields, including image classification, text categorization, and bioinformatics.\n",
        "  - Particularly valuable when dealing with high-dimensional data or complex decision boundaries.\n",
        "\n",
        "- **Considerations:**\n",
        "  - Choice of the kernel function is crucial and depends on the nature of the data.\n",
        "  - SVMs are sensitive to parameter tuning, especially the regularization parameter (C) and kernel parameters.\n",
        "\n",
        "- **Conclusion:**\n",
        "  - SVMs offer a robust and versatile approach for solving classification and regression problems, capable of handling both linear and nonlinear relationships in data. The kernel trick extends their applicability to a wide range of real-world scenarios."
      ],
      "metadata": {
        "id": "Jd-kotyQx-b2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Linear SVM:**\n",
        "\n",
        "1. **Decision Function:**\n",
        "   - Given a feature vector $ \\mathbf{x} $ and a weight vector $ \\mathbf{w} $, the decision function for a linear SVM is $ f(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b $, where $ b $ is the bias term.\n",
        "\n",
        "3. **Margin:**\n",
        "   - The margin between the hyperplane and the support vectors is $ \\frac{1}{||\\mathbf{w}||} $. The SVM aims to maximize this margin.\n",
        "\n",
        "4. **Loss Function:**\n",
        "   - The hinge loss is commonly used in SVMs:\n",
        "    $ \\text{Hinge Loss} = \\max(0, 1 - y \\cdot f(\\mathbf{x})) $\n",
        "     where $ y $ is the true class label (+1 or -1), and $ f(\\mathbf{x}) $ is the decision function.\n",
        "\n",
        "### **Nonlinear SVM (Kernel Trick):**\n",
        "\n",
        "1. **Kernel Function:**\n",
        "   - Introduce a kernel function $ K(\\mathbf{x}_i, \\mathbf{x}_j) $ to implicitly map the input data into a higher-dimensional space.\n",
        "\n",
        "2. **Decision Function with Kernel:**\n",
        "   - The decision function with the kernel becomes $ f(\\mathbf{x}) = \\sum_{i=1}^{N} \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b $, where $ \\alpha_i $ are the Lagrange multipliers.\n",
        "\n",
        "3. **Optimization Objective with Kernel:**\n",
        "   - The optimization problem includes the kernel term:\n",
        "     ![image.png](data:image/png;base64,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)\n",
        "\n",
        "4. **Kernel Functions:**\n",
        "   - Common kernel functions include:\n",
        "     - Linear Kernel: $ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i \\cdot \\mathbf{x}_j $\n",
        "     - Polynomial Kernel: $ K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i \\cdot \\mathbf{x}_j + c)^d $\n",
        "     - Radial Basis Function (RBF) Kernel: $ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\frac{||\\mathbf{x}_i - \\mathbf{x}_j||^2}{2\\sigma^2}\\right) $\n",
        "\n",
        "5. **Prediction:**\n",
        "   - Classify a new data point $ \\mathbf{x} $ based on the sign of $ f(\\mathbf{x}) $.\n",
        "\n",
        "In both cases, the optimization problems involve finding the optimal parameters that minimize the loss function while maintaining a suitable margin and controlling overfitting through regularization. The mathematics includes calculus, linear algebra, and optimization techniques like quadratic programming. The choice of the kernel function in nonlinear SVMs is crucial and depends on the data distribution."
      ],
      "metadata": {
        "id": "iFvhnCeg06EE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Digits dataset\n",
        "digits = datasets.load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a non-linear SVM classifier with RBF kernel\n",
        "svm_classifier = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
        "\n",
        "# Train the SVM classifier\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "predictions = svm_classifier.predict(X_test)\n",
        "\n",
        "# Print the results\n",
        "print('Predictions: ', predictions)\n",
        "print('\\nActual Labes: ' , y_test)\n",
        "print(\"\\nAccuracy:\", accuracy)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "print(\"\\nClassification Report:\\n\", classification_rep)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVi2B_SFyCBm",
        "outputId": "d53edd92-1284-4dec-caa5-84cca9fd377e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:  [6 9 3 7 2 1 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9\n",
            " 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4\n",
            " 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7\n",
            " 7 4 4 1 9 2 7 9 7 2 6 9 4 0 7 2 7 5 8 7 5 7 9 0 6 6 4 2 8 0 9 4 6 9 9 6 9\n",
            " 0 3 5 6 6 0 6 4 3 9 3 7 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7 2 2 3 9 8 0 3\n",
            " 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7\n",
            " 4 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 8 8 5 5 1 6 2 8 8 9 5 7 6 2 2 2 3 4 8 8\n",
            " 3 6 0 9 7 7 0 1 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5\n",
            " 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8\n",
            " 4 5 8 7 9 8 6 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4 3 8 3 5]\n",
            "\n",
            "Actual Labes:  [6 9 3 7 2 1 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9\n",
            " 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4\n",
            " 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7\n",
            " 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 7 0 6 6 4 2 8 0 9 4 6 9 9 6 9\n",
            " 0 3 5 6 6 0 6 4 3 9 3 9 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7 2 2 3 9 8 0 3\n",
            " 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7\n",
            " 4 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 8 8 5 5 1 6 2 8 8 9 9 7 6 2 2 2 3 4 8 8\n",
            " 3 6 0 9 7 7 0 1 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5\n",
            " 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8\n",
            " 4 5 8 7 9 8 5 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4 3 8 3 5]\n",
            "\n",
            "Accuracy: 0.9861111111111112\n",
            "\n",
            "Confusion Matrix:\n",
            " [[33  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 28  0  0  0  0  0  0  0  0]\n",
            " [ 0  0 33  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 34  0  0  0  0  0  0]\n",
            " [ 0  0  0  0 46  0  0  0  0  0]\n",
            " [ 0  0  0  0  0 46  1  0  0  0]\n",
            " [ 0  0  0  0  0  0 35  0  0  0]\n",
            " [ 0  0  0  0  0  0  0 33  0  1]\n",
            " [ 0  0  0  0  0  0  0  0 29  1]\n",
            " [ 0  0  0  0  0  1  0  1  0 38]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        33\n",
            "           1       1.00      1.00      1.00        28\n",
            "           2       1.00      1.00      1.00        33\n",
            "           3       1.00      1.00      1.00        34\n",
            "           4       1.00      1.00      1.00        46\n",
            "           5       0.98      0.98      0.98        47\n",
            "           6       0.97      1.00      0.99        35\n",
            "           7       0.97      0.97      0.97        34\n",
            "           8       1.00      0.97      0.98        30\n",
            "           9       0.95      0.95      0.95        40\n",
            "\n",
            "    accuracy                           0.99       360\n",
            "   macro avg       0.99      0.99      0.99       360\n",
            "weighted avg       0.99      0.99      0.99       360\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Gradient Boosting Algorithms\n",
        "\n",
        "\n",
        "**1. What is Gradient Boosting?**\n",
        "   - Gradient Boosting is a machine learning technique used for both classification and regression tasks.\n",
        "   - It builds a predictive model in a step-by-step manner, where each new model corrects the errors of the previous one.\n",
        "\n",
        "**2. How does it work?**\n",
        "   - Imagine you're trying to solve a problem, and initially, you make some predictions. These predictions might have errors.\n",
        "   - Gradient Boosting starts by building a simple model (usually a decision tree) that tries to correct those errors.\n",
        "\n",
        "**3. Learning from Mistakes:**\n",
        "   - The term \"Gradient\" in Gradient Boosting refers to the fact that the algorithm pays more attention to the mistakes (residual errors) made by the previous model.\n",
        "   - It calculates the gradient (slope) of the errors and adjusts the next model to reduce those errors.\n",
        "\n",
        "**4. Combining Weak Models:**\n",
        "   - Each new model added focuses on the mistakes of the combined ensemble of models before it.\n",
        "   - Weak models (simple ones) are usually employed, and many of them are combined to create a strong predictive model.\n",
        "\n",
        "**5. Iterative Process:**\n",
        "   - This process is repeated iteratively, with each new model improving upon the errors of the combined ensemble.\n",
        "   - The final prediction is made by combining the predictions of all the models.\n",
        "\n",
        "**6. Regularization:**\n",
        "   - To prevent overfitting (fitting the training data too closely and not generalizing well to new data), a technique called regularization is often applied.\n",
        "\n",
        "**7. Advantages:**\n",
        "   - Gradient Boosting is powerful and often yields highly accurate models.\n",
        "   - It can handle complex relationships in data.\n",
        "\n",
        "**8. Disadvantages:**\n",
        "   - It can be computationally expensive and might require tuning of parameters.\n",
        "   - Prone to overfitting if not properly regularized.\n",
        "\n",
        "**In a nutshell:**\n",
        "Gradient Boosting is like a team of players correcting each other's mistakes in a game. Each new player focuses on fixing the errors made by the team so far, gradually improving the overall performance. In machine learning terms, it's an ensemble method that combines weak models to create a strong predictive model by learning from previous mistakes."
      ],
      "metadata": {
        "id": "3Uet5qZq6eRe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation in mathematical aspect\n",
        "\n",
        "**1. Objective Function:**\n",
        "   - In Gradient Boosting, the goal is to minimize an objective function, often referred to as the loss function or cost function.\n",
        "   - For regression problems, a common loss function is the Mean Squared Error (MSE), while for classification problems, it can be the Cross-Entropy Loss.\n",
        "\n",
        "**2. Basic Idea:**\n",
        "   - Suppose we have a dataset with input-output pairs $(x_i, y_i)$ where $x_i$ is the input and $y_i$ is the corresponding true output.\n",
        "   - The objective is to find a model $F(x)$ that predicts $y$ as accurately as possible.\n",
        "\n",
        "**3. The Ensemble Model:**\n",
        "   - The initial prediction is usually a simple model, say $F_0(x)$, which might not be accurate.\n",
        "   - Subsequent models are added to correct the errors. Each new model $F_m(x)$ is trained to predict the residuals (errors) of the combined model $\\hat{y}_{m-1}(x)$.\n",
        "\n",
        "**4. Loss Function for Iterative Model:**\n",
        "   - Let's denote the current combined model as $\\hat{y}_{m-1}(x)$ and the residuals as $r_{m-1}(x) = y - \\hat{y}_{m-1}(x)$.\n",
        "   - The loss function for the $m$-th model is computed using the residuals: $L_m = \\text{Loss}(r_{m-1}(x), F_m(x))$.\n",
        "\n",
        "**5. Update Rule:**\n",
        "   - The new model is then trained to minimize this loss. The update rule is often derived using the gradient of the loss function with respect to the predicted values.\n",
        "   - For example, in regression problems with Mean Squared Error loss, the update rule for the $m$-th model might be $F_m(x) = F_{m-1}(x) + \\eta \\frac{\\partial L_m}{\\partial \\hat{y}_{m-1}(x)}$, where $\\eta$ is the learning rate.\n",
        "\n",
        "**6. Learning Rate:**\n",
        "   - The learning rate $\\eta$ controls how much we adjust our model in each iteration. It's a hyperparameter that needs to be tuned.\n",
        "\n",
        "**7. Regularization:**\n",
        "   - Regularization terms are often added to the loss function to prevent overfitting. These terms penalize complex models.\n",
        "\n",
        "**8. Final Prediction:**\n",
        "   - The final prediction is the sum of all individual predictions from each model: $\\hat{y}_{\\text{final}}(x) = F_0(x) + \\eta F_1(x) + \\eta F_2(x) + \\ldots + \\eta F_m(x)$.\n",
        "\n",
        "**In a nutshell:**\n",
        "Gradient Boosting involves iteratively adding models to correct the errors of the combined ensemble. The update rules are derived from the gradient of the loss function, and the final prediction is a weighted sum of all individual model predictions. Regularization is used to control model complexity, and the learning rate determines the step size in each iteration."
      ],
      "metadata": {
        "id": "EeNyktDl8Grn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import xgboost as xgb\n",
        "\n",
        "# Step 1: Load and Prepare Data (using Iris dataset)\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target, name='target')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Set Parameters\n",
        "params = {\n",
        "    'objective': 'multi:softmax',  # For multi-class classification\n",
        "    'num_class': 3,                # Number of classes in the target variable\n",
        "    'max_depth': 3,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 100,\n",
        "    'eval_metric': 'mlogloss'\n",
        "}\n",
        "\n",
        "# Step 3: Initialize and Train the Model\n",
        "model = xgb.XGBClassifier(**params)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate Accuracy and Display Results\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.2f}\\n')\n",
        "print('Classification Report:')\n",
        "print(classification_rep)\n",
        "print('\\nConfusion Matrix:')\n",
        "print(confusion_mat)\n",
        "\n",
        "# Additional: Display Predictions and Labels\n",
        "results = pd.DataFrame({'True Labels': y_test, 'Predicted Labels': y_pred})\n",
        "print('\\nPredictions and Labels:')\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D9kbGnR9Dj1",
        "outputId": "7fb3ffea-96fb-4715-e227-67ec44f9899e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.00\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Predictions and Labels:\n",
            "     True Labels  Predicted Labels\n",
            "73             1                 1\n",
            "18             0                 0\n",
            "118            2                 2\n",
            "78             1                 1\n",
            "76             1                 1\n",
            "31             0                 0\n",
            "64             1                 1\n",
            "141            2                 2\n",
            "68             1                 1\n",
            "82             1                 1\n",
            "110            2                 2\n",
            "12             0                 0\n",
            "36             0                 0\n",
            "9              0                 0\n",
            "19             0                 0\n",
            "56             1                 1\n",
            "104            2                 2\n",
            "69             1                 1\n",
            "55             1                 1\n",
            "132            2                 2\n",
            "29             0                 0\n",
            "127            2                 2\n",
            "26             0                 0\n",
            "128            2                 2\n",
            "131            2                 2\n",
            "145            2                 2\n",
            "108            2                 2\n",
            "143            2                 2\n",
            "45             0                 0\n",
            "30             0                 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. XGBoost Algorithm\n",
        "\n",
        "XGBoost, short for eXtreme Gradient Boosting, is a powerful machine learning algorithm that belongs to the family of gradient boosting methods. It was developed by Tianqi Chen and is widely used for classification and regression tasks. XGBoost has gained popularity and has become a go-to algorithm in various data science competitions and real-world applications due to its high performance and flexibility.\n",
        "\n",
        "Here's an explanation of the key components and features of XGBoost:\n",
        "\n",
        "1. **Gradient Boosting:**\n",
        "   - XGBoost is based on the gradient boosting framework, which is an ensemble learning technique where weak learners (usually decision trees) are trained sequentially, with each new tree trying to correct the errors of the previous ones.\n",
        "   - The term \"gradient\" in XGBoost refers to the optimization process where the algorithm minimizes a loss function by moving in the negative direction of the gradient (partial derivative) of the loss.\n",
        "\n",
        "2. **Regularization:**\n",
        "   - XGBoost incorporates regularization techniques to control overfitting, making the model more robust and generalizable.\n",
        "   - L1 (Lasso) and L2 (Ridge) regularization terms are added to the objective function, controlling the complexity of the individual trees.\n",
        "\n",
        "3. **Tree Pruning:**\n",
        "   - XGBoost grows trees depth-wise and applies pruning to reduce overfitting. It prunes branches of the tree that do not contribute significantly to the reduction in the loss function.\n",
        "\n",
        "4. **Parallelization:**\n",
        "   - XGBoost is designed to be computationally efficient. It supports parallel and distributed computing, making it faster than many other gradient boosting implementations.\n",
        "\n",
        "5. **Handling Missing Values:**\n",
        "   - XGBoost has a built-in mechanism to handle missing values in the dataset. It decides the direction in which missing values should go during the tree construction, treating them as just another value.\n",
        "\n",
        "6. **Cross-validation:**\n",
        "   - XGBoost supports k-fold cross-validation, allowing robust model evaluation by splitting the data into multiple subsets and training the model on different combinations of them.\n",
        "\n",
        "7. **Feature Importance:**\n",
        "   - XGBoost provides a feature importance score, which helps in understanding the significance of each feature in predicting the target variable. This can be useful for feature selection and interpretation.\n",
        "\n",
        "8. **Flexibility:**\n",
        "   - XGBoost can be used for both regression and classification tasks. It supports a variety of loss functions and allows customization of the learning task.\n",
        "\n",
        "**Why XGBoost is useful over other classifiers:**\n",
        "\n",
        "1. **Performance:**\n",
        "   - XGBoost often outperforms other machine learning algorithms in terms of accuracy and predictive power. It has been successful in winning numerous Kaggle competitions and is widely used in data science competitions.\n",
        "\n",
        "2. **Speed and Efficiency:**\n",
        "   - XGBoost is designed for efficiency and speed. Its implementation is highly optimized, making it faster than many other gradient boosting implementations.\n",
        "\n",
        "3. **Regularization:**\n",
        "   - The inclusion of L1 and L2 regularization terms helps control overfitting, making XGBoost more robust and less prone to memorizing noise in the training data.\n",
        "\n",
        "4. **Handling Missing Values:**\n",
        "   - XGBoost's ability to handle missing values internally is a significant advantage, eliminating the need for pre-processing steps to impute missing data.\n",
        "\n",
        "5. **Flexibility:**\n",
        "   - XGBoost is versatile and can be applied to various types of data and tasks. Its flexibility in handling different loss functions and problem types contributes to its wide usage.\n",
        "\n",
        "6. **Feature Importance:**\n",
        "   - The feature importance scores provided by XGBoost are valuable for understanding the impact of each feature on the model's predictions.\n",
        "\n",
        "In summary, XGBoost's combination of efficient implementation, regularization techniques, and flexibility makes it a popular choice for a wide range of machine learning tasks, often leading to superior performance compared to other classifiers."
      ],
      "metadata": {
        "id": "Zd0q1YkU801D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Convert to a DataFrame for better visualization (optional)\n",
        "columns = [f\"feature_{i+1}\" for i in range(X.shape[1])]\n",
        "df = pd.DataFrame(X, columns=columns)\n",
        "df['target'] = y\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the XGBoost model\n",
        "xg_model = xgb.XGBClassifier(objective='multi:softmax', num_class=3, seed=42)\n",
        "\n",
        "# Train the model\n",
        "xg_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = xg_model.predict(X_test)\n",
        "\n",
        "# Print the predicted labels and original labels\n",
        "print(\"Predicted labels:\", y_pred)\n",
        "print(\"Original labels:\", y_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print accuracy and other metrics\n",
        "print(\"\\nAccuracy:\", accuracy)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "print(\"\\nClassification Report:\\n\", class_report)\n",
        "\n",
        "# Display the parameters used in the XGBoost model\n",
        "print(\"\\nXGBoost Model Parameters:\")\n",
        "print(xg_model.get_params())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oTn_9xG3_3n",
        "outputId": "f69b02f9-e2e4-4bf9-882a-3e6dba70522e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted labels: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "Original labels: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "\n",
            "Accuracy: 1.0\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "XGBoost Model Parameters:\n",
            "{'objective': 'multi:softmax', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'num_class': 3, 'seed': 42}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Params available for Gradient boost and XGBoost\n",
        "\n",
        "### Gradient Boosting (Scikit-Learn)\n",
        "\n",
        "#### GradientBoostingRegressor and GradientBoostingClassifier\n",
        "\n",
        "1. `n_estimators`: Number of boosting stages to be run.\n",
        "   - Example options: integer (default=100)\n",
        "\n",
        "2. `learning_rate`: Shrinkage parameter to control the contribution of each tree.\n",
        "   - Example options: float (default=0.1)\n",
        "\n",
        "3. `max_depth`: Maximum depth of the individual trees.\n",
        "   - Example options: integer (default=3)\n",
        "\n",
        "4. `min_samples_split`: Minimum number of samples required to split an internal node.\n",
        "   - Example options: integer or float (default=2)\n",
        "\n",
        "5. `min_samples_leaf`: Minimum number of samples required to be at a leaf node.\n",
        "   - Example options: integer or float (default=1)\n",
        "\n",
        "6. `subsample`: Fraction of samples used for fitting the trees.\n",
        "   - Example options: float (default=1.0)\n",
        "\n",
        "7. `loss`: Loss function to be optimized.\n",
        "   - Example options: 'ls' (least squares regression), 'lad' (least absolute deviation), 'huber', etc.\n",
        "\n",
        "### XGBoost\n",
        "\n",
        "XGBoost has an extensive list of parameters. Here are some of them:\n",
        "\n",
        "1. `learning_rate` (or `eta`): Step size shrinkage to prevent overfitting.\n",
        "   - Example options: float (default=0.3)\n",
        "\n",
        "2. `n_estimators`: Number of boosting rounds (trees) to be run.\n",
        "   - Example options: integer (default=100)\n",
        "\n",
        "3. `max_depth`: Maximum depth of a tree.\n",
        "   - Example options: integer (default=6)\n",
        "\n",
        "4. `min_child_weight`: Minimum sum of instance weight (hessian) needed in a child.\n",
        "   - Example options: integer (default=1)\n",
        "\n",
        "5. `subsample`: Fraction of samples used for fitting the trees.\n",
        "   - Example options: float (default=1.0)\n",
        "\n",
        "6. `colsample_bytree`: Fraction of features used for fitting the trees.\n",
        "   - Example options: float (default=1.0)\n",
        "\n",
        "7. `reg_alpha` (or `alpha`): L1 regularization term on weights.\n",
        "   - Example options: float (default=0)\n",
        "\n",
        "8. `reg_lambda` (or `lambda`): L2 regularization term on weights.\n",
        "   - Example options: float (default=1)\n",
        "\n",
        "9. `gamma`: Minimum loss reduction required to make a further partition on a leaf node.\n",
        "   - Example options: float (default=0)\n",
        "\n",
        "10. `objective`: Specifies the learning task and corresponding objective function.\n",
        "    - Example options: 'reg:squarederror' (regression), 'binary:logistic' (binary classification), 'multi:softmax' (multiclass classification), etc.\n",
        "\n",
        "These are just a subset of the available parameters. The specific options and their default values may depend on the version of the libraries you are using. Always refer to the official documentation for the most accurate and up-to-date information."
      ],
      "metadata": {
        "id": "_ZKuvGn0ANVv"
      }
    }
  ]
}