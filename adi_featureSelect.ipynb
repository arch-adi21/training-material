{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP0z-bGoq9kr"
      },
      "source": [
        "Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMXR2CRcq9kt"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Math, Latex\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_theme(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJnv68luq9ku"
      },
      "source": [
        "## Feature Selection\n",
        "\n",
        "`sklearn.feature_selection` module has useful APIs to select features/reduce dimensionality, either to improve estimators' accuracy score or to boost their performance on very high-dimensional datasets.\n",
        "\n",
        "Top reasons to use feature selection are:\n",
        "\n",
        "\n",
        "* It enables the machine learning algorithm to train faster.\n",
        "\n",
        "* It reduces the complexity of a model and makes it easier to interpret.\n",
        "\n",
        "* It improves the accuracy of a model if the right subset is chosen.\n",
        "\n",
        "* It reduces overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gHi2NhYq9kv"
      },
      "source": [
        "#### **1.FILTER-BASED METHODS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiGBeDlGq9kv"
      },
      "source": [
        "##### 1.A. Variance Threshold\n",
        "\n",
        "* This transformer helps to keep only high variance features by providing a certain threshold.\n",
        "\n",
        "* Features with  variance greater or equal to threshold value are kept rest are removed.\n",
        "\n",
        "* By default, it removes any feature with same value i.e. 0 variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYHTZp97q9kv",
        "outputId": "235297f1-0bbb-4c91-a017-b9e088e17a87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.25 , 67.735])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "data = [{'age': 4, 'height': 96.0},\n",
        "        {'age': 1, 'height': 73.9},\n",
        "        {'age': 3,  'height': 88.9},\n",
        "        {'age': 2, 'height': 81.6}\n",
        "        ]\n",
        "\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "dv = DictVectorizer(sparse=False)\n",
        "\n",
        "data_transformed = dv.fit_transform(data)\n",
        "np.var(data_transformed, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aWAyIU4q9kw",
        "outputId": "04d662d4-757b-43d3-c09a-2457169d7466"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[96. ],\n",
              "       [73.9],\n",
              "       [88.9],\n",
              "       [81.6]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "vt = VarianceThreshold(threshold=5)\n",
        "\n",
        "data_new = vt.fit_transform(data_transformed)\n",
        "data_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdzOhcTwq9kw"
      },
      "source": [
        "As you may observe from output of above cell, the transformer has removed the age feature because its variance is below the threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig_mWrE6q9kw"
      },
      "source": [
        "##### 1.B. SelectKBest\n",
        "\n",
        "It selects k-highest scoring features based on a function and removes the rest of the features.\n",
        "\n",
        "Let's take an example of California Housing Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXj3iTIBq9kx"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
        "\n",
        "\n",
        "X_california, y_california = fetch_california_housing(return_X_y=True)\n",
        "X, y = X_california[:2000], y_california[:2000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDEDYP1Nq9kx"
      },
      "source": [
        "Let's select 3 most important features, since it is a regression problem, we can use only `mutual_info_regression` of `f_regression` scoring functions only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTmyUilTq9kx",
        "outputId": "f206bf57-ac0b-4025-913d-7c6ad1888f97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of feature-matrix before feature selection : (2000, 8)\n",
            "Shape of feature-matrix after feature selection : (2000, 3)\n"
          ]
        }
      ],
      "source": [
        "# mutual_info_regression is scoring method for linear regression method\n",
        "\n",
        "skb = SelectKBest(mutual_info_regression, k=3)\n",
        "X_new = skb.fit_transform(X, y)\n",
        "\n",
        "print(f'Shape of feature-matrix before feature selection : {X.shape}')\n",
        "print(f'Shape of feature-matrix after feature selection : {X_new.shape}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More about scoring functions\n",
        "\n",
        "Scoring functions, are metrics or measures used to evaluate the importance or relevance of individual features in a dataset with respect to the target variable. These functions assign scores to each feature based on certain criteria, and higher scores generally indicate more important or informative features.\n",
        "\n",
        "The choice of a scoring function depends on the type of machine learning task you are dealing with (classification or regression) and the nature of your data (categorical or continuous features).\n",
        "\n",
        "\n",
        "1. **For Regression Tasks:**\n",
        "   - `mutual_info_regression`: Measures the mutual information between each feature and the target variable in regression tasks.\n",
        "   - `f_regression`: Computes the F-value between each feature and the target variable. It assesses the linear relationship between features and the target in regression problems.\n",
        "\n",
        "2. **For Classification Tasks:**\n",
        "   - `mutual_info_classif`: Similar to `mutual_info_regression`, but tailored for classification tasks. Measures the mutual information between features and class labels.\n",
        "   - `f_classif`: Computes the ANOVA F-value for each feature with respect to class labels.\n",
        "\n",
        "3. **For Categorical Features:**\n",
        "   - `chi2`: Computes the chi-squared statistic between each non-negative feature and the target variable. It is useful for categorical feature selection in classification tasks.\n",
        "\n",
        "4. **Information Gain Measures:**\n",
        "   - `entropy`: Measures the entropy of a set of labels. Used in decision trees and information gain-based feature selection.\n",
        "\n",
        "5. **Correlation-Based Measures:**\n",
        "   - `pearson`: Computes the Pearson correlation coefficient between each feature and the target variable. Useful for linear relationships.\n",
        "\n",
        "These scoring functions help guide feature selection methods to identify the most relevant features for a particular machine learning task. Feature selection is crucial for improving model performance, reducing overfitting, and enhancing the interpretability of models by focusing on the most informative features."
      ],
      "metadata": {
        "id": "T-ImzaAxsSt9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT39VWvmq9kx"
      },
      "source": [
        "##### 1.C. SelectPercentile\n",
        "\n",
        "* This is very similar to `SelectKBest` from previous section, the only difference is, it selects top `percentile` of all features and drops the rest of features.\n",
        "\n",
        "* Similar to `SelecKBest`, it also uses a scoring function to decide the importance of features.\n",
        "\n",
        "Let's use the california housing price dataset for this API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gukIWaJ0q9kx",
        "outputId": "5dd35a69-4979-49f2-dd93-01fe62de219f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of feature-matrix before feature selection : (2000, 8)\n",
            "Shape of feature-matrix after feature selection : (2000, 3)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_selection import SelectPercentile\n",
        "\n",
        "sp = SelectPercentile(mutual_info_regression, percentile=30)\n",
        "X_new = sp.fit_transform(X, y)\n",
        "\n",
        "print(f'Shape of feature-matrix before feature selection : {X.shape}')\n",
        "print(f'Shape of feature-matrix after feature selection : {X_new.shape}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVgBzPlFq9ky"
      },
      "source": [
        "As you can see from above output, the transformed data now only has top 30 percentile of features, i.e only 3 out of 8 features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaLb76Gsq9ky",
        "outputId": "265b2e1e-6634-4f14-c142-604ab787b469"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['x0', 'x6', 'x7'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "skb.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sQ_pRSjq9ky"
      },
      "source": [
        "##### 1.D. GenericUnivariateSelect\n",
        "\n",
        "* Univariate feature selection is a type of feature selection method that evaluates the relationship between each individual feature and the target variable independently. In other words, it assesses the importance of each feature by considering only its individual impact on the target, without taking into account the interactions or dependencies between features.\n",
        "\n",
        "* It applies  univariate feature selection with a certain strategy, which is passed to the API via `mode` parameter.\n",
        "\n",
        "* The `mode` can take one of the following values :\n",
        "\n",
        "    * `percentile` (top percentage)\n",
        "\n",
        "    * `k_best` (top k)\n",
        "\n",
        "    * `fpr` (false positive rate)\n",
        "\n",
        "    * `fdr` (false discovery rate)\n",
        "\n",
        "    * `fwe` (family wise error rate)\n",
        "\n",
        "* If we want to accomplish the same objective as `SelectKBest`, we can use following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAG_brIkq9ky",
        "outputId": "6fc3a74d-f6d7-4a5d-daac-18098ee4a395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of feature-matrix before feature selection : (2000, 8)\n",
            "Shape of feature-matrix after feature selection : (2000, 3)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_selection import GenericUnivariateSelect\n",
        "\n",
        "gus = GenericUnivariateSelect(mutual_info_regression, mode='k_best', param = 3)\n",
        "X_new = gus.fit_transform(X,y)\n",
        "\n",
        "print(f'Shape of feature-matrix before feature selection : {X.shape}')\n",
        "print(f'Shape of feature-matrix after feature selection : {X_new.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D05dRrpdq9ky"
      },
      "source": [
        "#### **2.WRAPPER-BASED METHODS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0ka9a8Fq9ky"
      },
      "source": [
        "##### 2.A. Recursive Feature Elimination (RFE)\n",
        "\n",
        "* STEP 1 : Fits the model\n",
        "\n",
        "* STEP 2 : Ranks the features, afterwards it removes one or more features (depending upn `step` parameter)\n",
        "\n",
        "These two steps are repeated until desired number of features are selected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVBdI68Wq9ky"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "estimator = LinearRegression()\n",
        "\n",
        "selector = RFE(estimator, n_features_to_select=3, step=3)\n",
        "selector = selector.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbtU5R3-q9ky",
        "outputId": "032794f4-51b4-4fa9-8265-acafeb8cfe25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ True False False False False False  True  True]\n",
            "Rank of each feature is : [1 3 3 2 3 2 1 1]\n"
          ]
        }
      ],
      "source": [
        "# support_ attribute is a boolean array marking which features are selected\n",
        "print(selector.support_)\n",
        "\n",
        "# rank of each feature\n",
        "# if it's value is '1', then it is selected\n",
        "# features with rank 2 and onwards are ranked least.\n",
        "print(f'Rank of each feature is : {selector.ranking_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDhki9wgq9kz",
        "outputId": "dcb131b2-89bc-4553-b659-40a7c43c5ae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of feature-matrix before feature selection : (2000, 8)\n",
            "Shape of feature-matrix after feature selection : (2000, 3)\n"
          ]
        }
      ],
      "source": [
        "X_new = selector.transform(X)\n",
        "\n",
        "print(f'Shape of feature-matrix before feature selection : {X.shape}')\n",
        "print(f'Shape of feature-matrix after feature selection : {X_new.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na64nU7Rq9kz"
      },
      "source": [
        "##### 2.B. SelectFromModel\n",
        "\n",
        "* Selects desired number of important features (as specified with `max_features` parameter) above certain threshold of feature importance as obtained from the trained estimator.\n",
        "\n",
        "* The feature importance is obtained via `coef_`, `feature_importance_` or an `importance_getter` callable from the trained estimator.\n",
        "\n",
        "* The feature importance threshold can be specified either numerically or through string argument based on built-in heuristics such as `mean`, `median` and `float` multiples of these like `0.1*mean`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "NamMbSyOq9kz",
        "outputId": "a8ad8b6d-1602-43f0-bd70-8018101c1500"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "estimator = LinearRegression()\n",
        "estimator.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bXqpi5aq9kz",
        "outputId": "2114ccf7-e658-4b69-a2a6-d4ecfdc6fa7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients of features :\n",
            " [ 3.64048292e-01  5.56221906e-03  5.13591243e-02 -1.64474348e-01\n",
            "  5.90411479e-05 -1.64573915e-01 -2.17724525e-01 -1.85343265e-01]\n",
            "\n",
            "Intercept of features : -13.720597901356246\n",
            "\n",
            "Indices of top 3 features : [1 2 0]\n"
          ]
        }
      ],
      "source": [
        "print(f'Coefficients of features :\\n {estimator.coef_}')\n",
        "print()\n",
        "print(f'Intercept of features : {estimator.intercept_}')\n",
        "print()\n",
        "print(f'Indices of top {3} features : {np.argsort(estimator.coef_)[-3:]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6AH9F5Lq9kz",
        "outputId": "b2edad53-210e-4d19-c82f-394291e7abb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of feature-matrix before feature selection : (2000, 8)\n",
            "Shape of feature-matrix after feature selection : (2000, 3)\n"
          ]
        }
      ],
      "source": [
        "t = np.argsort(np.abs(estimator.coef_))[-3:]\n",
        "\n",
        "model = SelectFromModel(estimator, max_features=3, prefit=True)\n",
        "X_new = model.transform(X)\n",
        "\n",
        "print(f'Shape of feature-matrix before feature selection : {X.shape}')\n",
        "print(f'Shape of feature-matrix after feature selection : {X_new.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYVMZl3oq9kz"
      },
      "source": [
        "##### 2.C. SequentialFeatureSelection\n",
        "\n",
        "It performs feature selection by selecting or deselecting features one by one in a greedy manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NT0LdtdWq9kz"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SequentialFeatureSelector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c0t28FGq9kz",
        "outputId": "9c03cd3d-ed3d-41a6-df74-1eb9ce1d72bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ True False False False False  True  True False]\n",
            "CPU times: user 309 ms, sys: 2.56 ms, total: 311 ms\n",
            "Wall time: 317 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "estimator = LinearRegression()\n",
        "sfs = SequentialFeatureSelector(estimator, n_features_to_select=3)\n",
        "\n",
        "sfs.fit_transform(X, y)\n",
        "print(sfs.get_support())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FEj15gLq9kz"
      },
      "source": [
        "The features corresponding to True in the output of sfs.get_support() are selected. In this case,features 1, 6 and 7 are selected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Phoe_Gzcq9kz",
        "outputId": "5844ab88-abd7-4ba5-f08d-409d9dc75c8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ True False False False False  True  True False]\n",
            "CPU times: user 473 ms, sys: 245 ms, total: 718 ms\n",
            "Wall time: 464 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "estimator = LinearRegression()\n",
        "sfs = SequentialFeatureSelector(\n",
        "    estimator, n_features_to_select=3, direction='backward')\n",
        "\n",
        "sfs.fit_transform(X, y)\n",
        "print(sfs.get_support())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbY595nZq9k0"
      },
      "source": [
        "A couple of observations:\n",
        "* Both `forward` and `backward` selection methods select the same featurers.\n",
        "\n",
        "* The `backward` selection method takes longer than `forward` selection method.\n",
        "\n",
        "From above examples, we can observe that depending upon number of features, `SFS` can accomplish feature selection in different periods forwards and backwards.\n"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "32f02dc107888b055323539767db1f9cf579f03b0ed3a3ace7986ed2d38433ec"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('tensorflow')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}